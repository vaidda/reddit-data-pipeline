services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    networks:
      - default

  kafka:
    image: bitnami/kafka:3.5.1
    container_name: kafka
    ports:
      - "9092:9092"
      - "9999:9999"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      JMX_PORT: 9999
      KAFKA_JMX_OPTS: >
        -Dcom.sun.management.jmxremote
        -Dcom.sun.management.jmxremote.port=9999
        -Dcom.sun.management.jmxremote.rmi.port=9999
        -Dcom.sun.management.jmxremote.authenticate=false
        -Dcom.sun.management.jmxremote.ssl=false
        -Djava.rmi.server.hostname=kafka
    depends_on:
      - zookeeper
    networks:
      - default

  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5432:5432"
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - default

  airflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: airflow
    env_file:
      - .env
    volumes:
      - .:/app
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - kafka
    environment:
      NLTK_DATA: ${NLTK_DATA}
      PYTHONPATH: /app
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    entrypoint: >
      bash -c "airflow db migrate &&
      airflow users create --username $AIRFLOW_ADMIN_USERNAME --password $AIRFLOW_ADMIN_PASSWORD --firstname Air --lastname Flow --role Admin --email $AIRFLOW_ADMIN_EMAIL || true &&
      airflow scheduler & airflow webserver"
    networks:
      - default

  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    ports:
      - "9308:9308"
    environment:
      KAFKA_SERVER: kafka:9092
      KAFKA_VERSION: "3.5.1"
      USE_JMX: "true"
      JMX_PORT: "9999"
    depends_on:
      - kafka
    networks:
      - default

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - default

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    networks:
      - default

  superset:
    build:
      context: .
      dockerfile: docker/Dockerfile.superset
    image: apache/superset
    container_name: superset
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_DATABASE_URI: ${SUPERSET_DATABASE_URI}
    ports:
      - "8088:8088"
    depends_on:
      - postgres
    networks:
      - default
    volumes:
      - ./superset:/app/superset_home
    env_file:
      - .env
    command: >
      /bin/bash -c "
        superset db upgrade &&
        superset fab create-admin --username $SUPERSET_ADMIN_USERNAME --firstname $SUPERSET_ADMIN_FIRSTNAME --lastname $SUPERSET_ADMIN_LASTNAME --email $SUPERSET_ADMIN_EMAIL --password $SUPERSET_ADMIN_PASSWORD &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger"

  app:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: app
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      - kafka

volumes:
  pgdata:

networks:
  default:
    name: api_pipeline_network
